\documentclass[aps,pre,twocolumn,showpacs,superscriptaddress,floatfix]{revtex4-2}

% Packages
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{color}

% Custom commands for physics notation
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\braket}[2]{\langle #1|#2\rangle}
\newcommand{\expected}[1]{\left\langle #1 \right\rangle}

\begin{document}

% --- TITLE & AUTHOR ---
\title{Thermodynamic Phase Transitions in Large Language Models:\\A Spin Glass Perspective on Hallucination}

\author{Marcus A. P. Roriz}
\email{marcus.roriz@google.com} % Adjust as necessary
\affiliation{Google AI Red Team, Google, Mountain View, CA, USA}
\affiliation{Department of Physics, Federal University of Goiás (UFG), Goiânia, Brazil}

\date{\today}

% --- ABSTRACT ---
\begin{abstract}
Large Language Models (LLMs) exhibit a sudden degradation in factual accuracy known as "hallucination," typically attributed to statistical sampling errors. We propose a fundamental thermodynamic framework where hallucination corresponds to a disorder-induced phase transition in the model's energy landscape. By mapping the Transformer's self-attention mechanism to a continuous Modern Hopfield Network (Dense Associative Memory), we derive an effective Hamiltonian governing the inference dynamics. We identify a critical temperature $T_c$, determined by the spectral properties of the weight matrices, which separates a ferromagnetic \textit{retrieval phase} (factual accuracy) from a spin-glass \textit{confabulation phase} (hallucination). Our mean-field theoretical analysis, supported by numerical simulations, suggests that hallucination is an emergent property of operating high-capacity neural networks near the edge of chaos, distinct from simple paramagnetic noise.
\end{abstract}

\maketitle

% --- I. INTRODUCTION ---
\section{Introduction}
The rapid advancement of Large Language Models (LLMs) based on the Transformer architecture \cite{vaswani2017} has revolutionized natural language processing. However, a persistent failure mode remains: hallucination---the generation of syntactically coherent but factually incorrect information. While current mitigation strategies focus on Reinforcement Learning from Human Feedback (RLHF) or retrieval-augmented generation (RAG), a first-principles understanding of \textit{why} these errors emerge remains elusive.

In this work, we approach the problem through the lens of Statistical Mechanics. We posit that a pre-trained LLM acts as a disordered system with "frozen" interactions (weights) determined by the training data. During inference, the "temperature" hyperparameter controls the stochasticity of the token generation. We demonstrate that the transition from factual recall to hallucination is isomorphic to the phase transition from a ferromagnetic state to a spin-glass state in disordered magnetic systems \cite{mezard1987}.

% --- II. THE FORMALISM ---
\section{Formalism: Attention as Energy Minimization}

We focus on the self-attention mechanism, the core component of Transformers. It has been recently shown that the update rule of a continuous Modern Hopfield Network is mathematically equivalent to the attention mechanism \cite{ramsauer2020}.

Let $\mathbf{x} \in \mathbb{R}^d$ be the state vector (query embedding) and $\{ \boldsymbol{\xi}^\mu \}_{\mu=1}^P$ be the set of stored patterns (keys/values obtained from training). The energy function (Hamiltonian) of the system is given by:

\begin{equation}
    \mathcal{H}(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T \mathbf{x} - \frac{1}{\beta} \ln \left( \sum_{\mu=1}^P e^{\beta \boldsymbol{\xi}^\mu \cdot \mathbf{x}} \right)
    \label{eq:hamiltonian}
\end{equation}

where $\beta = 1/T$ is the inverse temperature. The first term represents a confinement potential preventing divergence, while the second term (Log-Sum-Exp) represents the interaction energy with stored memories.

The dynamics of the system during inference seek to minimize this energy. The update rule for the state $\mathbf{x}$ at step $t+1$ is given by the gradient descent on $\mathcal{H}$:

\begin{equation}
    \mathbf{x}_{t+1} = \nabla_{\mathbf{x}} \left( \frac{1}{\beta} \ln \sum_\mu e^{\beta \boldsymbol{\xi}^\mu \cdot \mathbf{x}_t} \right) = \sum_{\mu=1}^P \text{softmax}(\beta \boldsymbol{\xi}^\mu \cdot \mathbf{x}_t) \boldsymbol{\xi}^\mu
    \label{eq:attention_update}
\end{equation}

Eq. (\ref{eq:attention_update}) is exactly the self-attention operation where queries $\mathbf{x}$ attend to keys $\boldsymbol{\xi}^\mu$.

% --- III. MEAN FIELD THEORY ---
\section{Mean Field Theory of Hallucination}

To analyze the stability of "factual" states, we introduce the overlap order parameter (magnetization) $m_\mu$, which measures the correlation between the current state and a specific stored memory $\mu$:
\begin{equation}
    m_\mu = \expected{ \boldsymbol{\xi}^\mu \cdot \mathbf{x} }
\end{equation}

In the thermodynamic limit ($N, P \to \infty$ with load $\alpha = P/N$ finite), the system exhibits distinct phases determined by the temperature $T$.

\subsection{The Retrieval Phase (Factuality)}
For $T < T_c$, the energy landscape is dominated by deep global minima corresponding to single patterns $\boldsymbol{\xi}^\mu$. Here, $m_\mu \approx 1$ for the target pattern and $m_{\nu \neq \mu} \approx 0$. The system retrieves the correct fact stored in the weights.

\subsection{The Spin Glass Phase (Hallucination)}
As $T$ increases or the memory load $\alpha$ exceeds a critical capacity, spurious local minima emerge. These minima are linear combinations of multiple patterns:
\begin{equation}
    \mathbf{x}_{metastable} = \sum_{\mu} c_\mu \boldsymbol{\xi}^\mu
\end{equation}
In this phase, the system does not converge to a single memory (fact) but settles into a "glassy" state where it mixes features of multiple memories. Physically, the overlap with the ground truth drops ($m_{target} \to 0$), yet the Edwards-Anderson parameter $q_{EA}$ remains non-zero:
\begin{equation}
    q_{EA} = \frac{1}{N} \sum_i \expected{x_i}^2 > 0
\end{equation}
This indicates the system is \textit{frozen} in a configuration that is internally consistent but factually wrong relative to the query. This is the precise thermodynamic definition of hallucination.

\subsection{Critical Temperature derivation}
Linearizing the update rule around the paramagnetic solution ($\mathbf{x} \approx \mathbf{0}$), we find the instability condition. The critical temperature $T_c$ is related to the spectral radius of the covariance matrix of the stored patterns $\mathbf{C} = \frac{1}{P}\sum \boldsymbol{\xi}^\mu (\boldsymbol{\xi}^\mu)^T$:

\begin{equation}
    T_c \approx \lambda_{max}(\mathbf{C})
\end{equation}

If the temperature of the Softmax layer exceeds this spectral bound, the attention mechanism cannot sustain coherence with the relevant memory subspace.

% --- IV. RESULTS & DISCUSSION ---
\section{Results and Discussion}

We performed numerical simulations of the update rule Eq. (\ref{eq:attention_update}) for a system with $N=128$ and load $\alpha=2.0$.

\begin{figure}[h]
    \centering
    % Placeholder for the image generated in Python
    \includegraphics[width=0.9\linewidth]{phase_diagram.png}
    \caption{Phase diagram of the attention mechanism. The black line (Target Overlap) represents factual accuracy. The red dashed line (Max Overlap) represents internal consistency. The region between $T \approx 0.15$ and $T \approx 0.35$ defines the \textit{Hallucination Phase}, where the model loses the target fact but remains confident in a spurious memory.}
    \label{fig:phase_diagram}
\end{figure}

Our results (Fig. \ref{fig:phase_diagram}) clearly show three regimes:
\begin{enumerate}
    \item \textbf{Retrieval Phase ($T < 0.15$):} The model perfectly reconstructs the target memory.
    \item \textbf{Hallucination Phase ($0.15 < T < 0.35$):} The target overlap collapses, but the system converges to a high-energy metastable state. This corresponds to plausible but incorrect generation.
    \item \textbf{Paramagnetic Phase ($T > 0.35$):} The system enters a high-noise regime, generating incoherent text.
\end{enumerate}

% --- V. CONCLUSION ---
\section{Conclusion}
We have established a mapping between LLM hallucination and spin glass phase transitions. The phenomenon is not merely a defect of training but a thermodynamic necessity of high-dimensional associative memories operating at finite temperature. We propose that "hallucination" is simply the system falling into a spin-glass state. Future work should focus on spectral regularization techniques to increase $T_c$, thereby expanding the stable retrieval phase.

\begin{acknowledgments}
M.A.P.R. acknowledges support from the Google AI Red Team and the Physics Department at UFG.
\end{acknowledgments}

% --- BIBLIOGRAPHY ---
\begin{thebibliography}{9}
\bibitem{vaswani2017} A. Vaswani et al., \textit{Attention is all you need}, NeurIPS (2017).
\bibitem{mezard1987} M. Mézard, G. Parisi, and M. A. Virasoro, \textit{Spin glass theory and beyond}, World Scientific (1987).
\bibitem{ramsauer2020} H. Ramsauer et al., \textit{Hopfield Networks is All You Need}, ICLR (2021).
\end{thebibliography}

\end{document}